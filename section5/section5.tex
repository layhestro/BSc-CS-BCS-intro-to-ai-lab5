\section{Reflection Questions}

\subsection{Question 5.1: Model Deployment for Mobile}

\textbf{Choice:} Original CNN (2-layer)

\textbf{Justification:} The 2-layer CNN offers the best balance for mobile deployment:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Parameters} & \textbf{Val Accuracy} \\ \hline
Best MLP (Dropout) & 535,818 & 98.31\% \\ \hline
CNN (2 layers) & 421,642 & 99.03\% \\ \hline
Deeper CNN (3 layers) & 458,570 & 99.29\% \\ \hline
\end{tabular}
\caption{Model comparison for mobile deployment}
\end{table}

The 2-layer CNN is 21\% smaller than the MLP and 8\% smaller than the deeper CNN, while achieving 99.03\% accuracy (only 0.26\% less than the deeper CNN). Smaller size means faster downloads, less storage, lower memory consumption, faster inference, and better battery life, all critical for mobile devices.

\subsection{Question 5.2: Critique of "More is Better"}

This approach is really flawed:

\begin{enumerate}
    \item \textbf{Overfitting}: More capacity makes memorization easier. Our baseline achieved 99.79\% training but only 97.74\% validation (2.05\% gap), showing overfitting. Adding more layers would worsen this.
    
    \item \textbf{Diminishing returns}: Our deeper CNN experiment proved this. Adding a third layer improved accuracy by only 0.26\% despite 8.8\% more parameters.
    
    \item \textbf{Bayes error rate}: Every dataset has irreducible error from noise and ambiguity. Pursuing 100\% accuracy means overfitting to noise, not achieving perfection.
    
    \item \textbf{Better alternatives}: Dropout (98.31\%) and early stopping (97.90\%) achieved better generalization than simply training a large model (97.74\%).
\end{enumerate}

\textbf{Better approach:} Analyze train-validation gap, apply regularization (dropout, early stopping), use appropriate architectures (CNNs for images), and accept that 100\% accuracy is neither achievable nor desirable.

\subsection{Question 5.3: Strategies for Avoiding Overfitting}

\subsubsection{Strategy 1: Dropout Regularization}

\textbf{Implementation:} Add dropout layers that randomly deactivate neurons during training ($rate=0.3$).

\textbf{Evidence:} Improved validation accuracy from 97.74\% to 98.31\% and reduced train-val gap from 2.05\% to 1.08\%.

\textbf{Why it works:} Forces the network to learn redundant representations, neurons cannot rely on specific others always being present, so the network learns more robust, generalizable features.

\subsubsection{Strategy 2: Early Stopping}

\textbf{Implementation:} Monitor validation loss and stop when it plateaus for a patience period (5 epochs).

\textbf{Evidence:} Training stopped at epoch 11 instead of 50 (78\% reduction) with slight improvement (97.90\% vs 97.74\%).

\textbf{Why it works:} Networks initially learn general patterns, then progressively fit training-specific noise. Early stopping catches the model before memorization begins, as evidenced by our baseline's validation loss increasing after epoch 10.

\textbf{Recommendation:} Use both together, dropout for better generalization, early stopping for efficiency. Our experiments suggest combining them would yield optimal results.