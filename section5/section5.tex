\section{Reflection Questions}

\subsection{Question 5.1: Model Deployment for Mobile}

\textbf{Choice:} Original CNN (2-layer)

\textbf{Justification:} The 2-layer CNN offers the best balance for mobile:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Parameters} & \textbf{Val Accuracy} \\ \hline
Best MLP (Dropout) & 535,818 & 98.31\% \\ \hline
CNN (2 layers) & 421,642 & 99.03\% \\ \hline
Deeper CNN (3 layers) & 458,570 & 99.29\% \\ \hline
\end{tabular}
\caption{Model comparison for mobile deployment}
\end{table}

The 2-layer CNN is 21\% smaller than the MLP and 8\% smaller than the deeper CNN, while achieving 99.03\% accuracy (only 0.26\% less than the deeper CNN). Smaller size means faster downloads, less storage, lower memory consumption, faster inference, and better battery life, all critical for mobile devices.

\subsection{Question 5.2: Critique of "More is Better"}

This approach has some serious problems:

\begin{enumerate}
    \item \textbf{Overfitting risk}: More layers means the model can memorize training data more easily. Our baseline got 99.79\% training accuracy but only 97.74\% validation, showing it was already memorizing. Adding more would make this worse.
    
    \item \textbf{Diminishing returns}: When I added a third CNN layer, accuracy only improved by 0.26\% despite adding 8.8\% more parameters. The improvement wasn't worth the cost.
    
    \item \textbf{Chasing perfection}: No dataset is perfect, there's always some noise or ambiguous examples. Trying to get 100\% accuracy just means you're fitting to the noise.
    
    \item \textbf{Better solutions exist}: Dropout got us to 98.31\% validation accuracy (better than baseline's 97.74\%) without making the model bigger.
\end{enumerate}

\textbf{Better approach:} Look at the gap between training and validation accuracy, use regularization techniques like dropout or early stopping, and accept that you can't (and shouldn't) get 100\% accuracy.

\subsection{Question 5.3: Strategies for Avoiding Overfitting}

\subsubsection{Strategy 1: Dropout}

Dropout randomly turns off 30\% of neurons during each training step. When I added this to the MLP, validation accuracy improved from 97.74\% to 98.31\%, and the gap between training and validation dropped from 2.05\% to 1.08\%. 

The idea is that neurons can't rely on other specific neurons always being there, so they're forced to learn more general patterns instead of memorizing. It's like studying without knowing which topics will be on the exam,you learn everything more thoroughly instead of just memorizing specific answers.

\subsubsection{Strategy 2: Early Stopping}

This one just stops training when validation loss hasn't improved for 5 epochs in a row. The model stopped at epoch 11 instead of running all 50 epochs (saving 78\% of training time) and actually got slightly better results (97.90\% vs 97.74\%).

Looking at the baseline graph, you can see validation loss starts going up after epoch 10 while training loss keeps dropping, that's exactly when the model starts memorizing instead of learning. Early stopping just catches it at the right moment before that happens.

\subsubsection{My Recommendation}

Between the two, I'd say use dropout when you want the best accuracy, and early stopping when you want to save time. But honestly, combining both would probably work even better.